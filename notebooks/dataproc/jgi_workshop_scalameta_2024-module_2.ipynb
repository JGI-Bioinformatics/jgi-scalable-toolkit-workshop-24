{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f833330-3649-4132-acb6-90505e202912",
   "metadata": {
    "id": "515c9dde"
   },
   "source": [
    "#### cluster-specific setup, adjust this according to your own ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b21bac4",
   "metadata": {
    "id": "3b21bac4"
   },
   "outputs": [],
   "source": [
    " #start a new Spark session, necessary for all Spark-related operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "PFAM_HMM_PATH = \"/Pfam-A.hmm\" # should also contain all the hmmpressed files (will be captured using glob)\n",
    "\n",
    "# 'spark' and 'sc' are the canonical variable names for the SparkSession and SparkContext objects\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.files\", \",\".join([PFAM_HMM_PATH + add for add in [\"\", \".h3f\", \".h3i\", \".h3m\", \".h3p\"]]))\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9d49a",
   "metadata": {
    "id": "5cd9d49a"
   },
   "outputs": [],
   "source": [
    "# point here to where the workshop's data folder is located\n",
    "from os import path\n",
    "\n",
    "DATA_FOLDER = \"gs://zw_axolotl/jgi_workshop_2024/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8e1ae",
   "metadata": {
    "id": "afa8e1ae"
   },
   "outputs": [],
   "source": [
    "# point here to where we will store results from the activity (make sure it's empty and readable!)\n",
    "\n",
    "RESULT_FOLDER = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37f304",
   "metadata": {
    "id": "0d37f304"
   },
   "outputs": [],
   "source": [
    "# helper variable to set minimum repartition of the data (to use up all executors)\n",
    "\n",
    "MIN_PARTITIONS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TtNU0Y_c2Lex",
   "metadata": {
    "id": "TtNU0Y_c2Lex"
   },
   "outputs": [],
   "source": [
    "# to avoid needing to mount a Gdrive folder, we will copy an existing parquet for the taxonomy table from module 1\n",
    "#spark.read.parquet(path.join(DATA_FOLDER, \"df_taxonomy.pq\")).write.mode(\"overwrite\").parquet(path.join(RESULT_FOLDER, \"df_taxonomy.pq\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1843403",
   "metadata": {
    "id": "f1843403"
   },
   "source": [
    "#### variables setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3546a",
   "metadata": {
    "id": "c6b3546a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F # contain frequently-used functions\n",
    "from pyspark.sql import types as T # for specifying schema datatypes, etc\n",
    "\n",
    "# basic python libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21d9b5",
   "metadata": {
    "id": "aa21d9b5"
   },
   "source": [
    "### The dataset\n",
    "The full dataset comprises 13,223 \"Species representative\" genomes of the MGnify Marine V2.0 catalogue (https://www.ebi.ac.uk/metagenomics/genome-catalogues/marine-v2-0).\n",
    "\n",
    "Each genome folder contain:\n",
    "- Fasta files of the contig sequences: *.fna\n",
    "- GFF3 files of gene-calling annotations: *.gff\n",
    "- GFF3 files of BGC-calling annotations (using their in-house pipeline): *_sanntis.gff\n",
    "\n",
    "The data folder \"genomes_all\" is organized by Phylum->Genus->Genome, for example:\n",
    "\n",
    "- genomes_all\n",
    " - Actinomycetota\n",
    "   - Streptomycetes\n",
    "     - MYM000001\n",
    "       - MYM000001.fna\n",
    "       - MYM000001.gff\n",
    "       - MYM000001_sanntis.gff\n",
    "   - Microbacteria\n",
    "   - ...\n",
    " - Pseudomonatota\n",
    "   - ...\n",
    " - ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f42908",
   "metadata": {
    "id": "e8f42908"
   },
   "outputs": [],
   "source": [
    "# Load the metadata (and persist it) for the entire dataset, as we will be using it later on\n",
    "\n",
    "metadata_sdf = (spark.read\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"samplingRatio\", 0.0001)\n",
    "    .csv(path.join(DATA_FOLDER, \"mgnify-marine-v2-0.tsv\"))\n",
    ").filter(F.col(\"Genome\") == F.col(\"Species_rep\")) # takes only the species representative genomes from the full 5X,XXX genomes\n",
    "\n",
    "metadata_sdf.createOrReplaceTempView(\"metadata\")\n",
    "metadata_sdf.persist()\n",
    "metadata_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X-Qx37tWaBK9",
   "metadata": {
    "id": "X-Qx37tWaBK9"
   },
   "outputs": [],
   "source": [
    "taxonomy_pq_path = path.join(RESULT_FOLDER, \"df_taxonomy.pq\") # from module_1\n",
    "\n",
    "# we will load the taxonomy as SQL, after filtering it only for the 13,223 species rep genomes (via Join)\n",
    "spark.read.parquet(taxonomy_pq_path).createOrReplaceTempView(\"taxonomy\")\n",
    "taxonomy_sdf = spark.sql(\"SELECT taxonomy.* FROM taxonomy JOIN metadata ON taxonomy.Genome=metadata.Genome\")\n",
    "taxonomy_sdf.createOrReplaceTempView(\"taxonomy\")\n",
    "\n",
    "taxonomy_sdf.persist()\n",
    "taxonomy_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_u9Y8E4-irJ-",
   "metadata": {
    "id": "_u9Y8E4-irJ-"
   },
   "outputs": [],
   "source": [
    "# we will create a new table, to lookup the number of genomes + species included in a single genus\n",
    "genus_stat_sdf = spark.sql((\n",
    "    \"SELECT Phylum, Genus, count(Genome) as num_genomes, count(distinct Species) as num_species\"\n",
    "    \" FROM taxonomy\"\n",
    "    \" GROUP BY Phylum, Genus\"\n",
    "))\n",
    "\n",
    "genus_stat_sdf.createOrReplaceTempView(\"genus_stat\")\n",
    "genus_stat_sdf.persist().count()\n",
    "\n",
    "# here is what the table looks like\n",
    "genus_stat_sdf.orderBy(\"num_genomes\", ascending=False).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Lbzxnvfj_2G",
   "metadata": {
    "id": "-Lbzxnvfj_2G"
   },
   "source": [
    "#### For this activity, let's look up a total of <10 genomes from two different genera, all with unique species\n",
    "\n",
    "In this case, we will arbitarily choose <i>Streptomyces</i> and <i>Kitasatospora</i>, two related genera that were known for their biosynthetic potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AIEvrhhuj-yW",
   "metadata": {
    "id": "AIEvrhhuj-yW"
   },
   "outputs": [],
   "source": [
    "# First, we will look up the correct naming of our genera of choice, along with their genomes/species count\n",
    "# we can use the wild card character '%' to search using the SQL query\n",
    "\n",
    "spark.sql((\n",
    "    \"SELECT * FROM genus_stat\"\n",
    "    \" WHERE Genus LIKE '%Strepto%' OR Genus LIKE '%Kita%'\"\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Opg7Qa-8hBVT",
   "metadata": {
    "id": "Opg7Qa-8hBVT"
   },
   "outputs": [],
   "source": [
    "# then let's encode and specify the genera to be selected, complete with its Phylum\n",
    "included_genera = [\n",
    "    (\"p__Actinomycetota\", \"g__Streptomyces\"),\n",
    "    (\"p__Actinomycetota\", \"g__Kitasatospora\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qXq88xAIcJ-b",
   "metadata": {
    "id": "qXq88xAIcJ-b"
   },
   "outputs": [],
   "source": [
    "# this is the function that will later on be used to generate\n",
    "# the list of folder paths of each genus\n",
    "# all unknown genera ('g__') is combined into a folder called 'unknown_genus'\n",
    "\n",
    "def generate_folder_paths(list_of_genera, file_extension=None):\n",
    "    return [path.join(DATA_FOLDER, \"genomes_all\", phylum.split(\"__\")[1], genus.split(\"__\")[1] if genus != \"g__\" else \"unknown_genus\") for phylum, genus in list_of_genera]\n",
    "\n",
    "# for example, here is the folder we selected\n",
    "generate_folder_paths(included_genera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81dcae",
   "metadata": {
    "id": "6e81dcae"
   },
   "source": [
    "### Activity 2-1: Parsing FASTA files from scratch with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65846124",
   "metadata": {
    "id": "65846124"
   },
   "source": [
    "First, let's make a FASTA parser function that takes: a **file path string** and a **nucleotide FASTA text (can be multi-sequences)** and output list of sequences and its IDs+headers, along with its original file path, as follow:\n",
    "\n",
    "**Input:** ('example_fasta.fna',\n",
    "'\n",
    "&gt;seq_1 desc_1\n",
    "<br/>ATGCATGCATGC<br/>ATGCATGCATGC\n",
    "\n",
    "&gt;seq_2 desc_2\n",
    "<br/>AAAAAAAAAAAA\n",
    "<br/>TTTTTTTTTTTT\n",
    "'\n",
    "\n",
    "**Output:**\n",
    "\n",
    "[<br/>\n",
    "['example_fasta.fna', 'seq_1', 'desc_1', 'ATGCATGCATGCATGCATGCATGC'],<br/>\n",
    "['example_fasta.fna', 'seq_2', 'desc_2', 'AAAAAAAAAAAATTTTTTTTTTTT']\n",
    "<br/>]\n",
    "\n",
    "This function will be called later in combination with Spark's parallel file parsing function 'wholeTextFiles()' to produce a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac78d31",
   "metadata": {
    "id": "5ac78d31"
   },
   "outputs": [],
   "source": [
    "# this code is derived from chatGPT\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_fasta(file_path, fasta_str):\n",
    "    sequences = []\n",
    "    fasta_entries = fasta_str.strip().split('>')\n",
    "\n",
    "    for entry in fasta_entries:\n",
    "        if entry:\n",
    "            lines = entry.split('\\n')\n",
    "            header = lines[0]\n",
    "            sequence = ''.join(lines[1:])\n",
    "\n",
    "            match = re.match(r'(\\S+)\\s*(.*)', header)\n",
    "            if match:\n",
    "                seq_id = match.group(1)\n",
    "                description = match.group(2)\n",
    "                sequences.append([file_path, seq_id, description, sequence])\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# test\n",
    "file_path = \"example.fasta\"\n",
    "fasta_str = \"\"\">seq1 Description of seq1\n",
    "ATGCGTA\n",
    ">seq2 Another description\n",
    "GCTAGCTAGCTA\n",
    ">seq3 Yet another description\n",
    "TGCATGCA\"\"\"\n",
    "\n",
    "%time parse_fasta(file_path, fasta_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d7eb2",
   "metadata": {
    "id": "564d7eb2"
   },
   "source": [
    "#### Parsing using Spark's wholeTextFiles()\n",
    "\n",
    "with Spark, we can use the wholeTextFiles() to scan and load text files into RDDs of string,\n",
    "then pass it down to our parser function. This works well for smaller files that can fit each\n",
    "into the worker's memory. See https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.wholeTextFiles.html\n",
    "\n",
    "The function takes file paths as a single string, separated by <b>','</b>. You can use glob-style wildcard <b>'*'</b> to capture multiple files with the same pattern.\n",
    "\n",
    "If you are parsing fewer, but big files, you would use https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OZ3KKt2kml69",
   "metadata": {
    "id": "OZ3KKt2kml69"
   },
   "outputs": [],
   "source": [
    "# let's generate the input fasta paths (using the function we wrote above, combining it with the included genera)\n",
    "input_fasta_paths = [path.join(folder_path, \"*/*.fna\") for folder_path in generate_folder_paths(included_genera)]\n",
    "input_fasta_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57c80d",
   "metadata": {
    "id": "0f57c80d"
   },
   "outputs": [],
   "source": [
    "# flatMap() is used to flatten the list of lists into single list (i.e., row in the dataframe)\n",
    "from_spark_df = (sc.wholeTextFiles(\",\".join(input_fasta_paths)).flatMap(\n",
    "    lambda tup: parse_fasta(tup[0], tup[1]) # wholeTextFiles output: RDD of (file_path, file_content_string)\n",
    ").toDF([\"file_path\", \"seq_id\", \"desc\", \"sequence\"])\n",
    "    .withColumn(\"length\", F.length(F.col(\"sequence\"))) # add a new column: length\n",
    ")\n",
    "\n",
    "from_spark_df.persist()\n",
    "%time from_spark_df.count()\n",
    "\n",
    "from_spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a27c4",
   "metadata": {
    "id": "bc9a27c4"
   },
   "outputs": [],
   "source": [
    "# let's check the statistics\n",
    "from_spark_df.select(\n",
    "    F.countDistinct(F.col(\"file_path\")).alias(\"num_files\"),\n",
    "    F.count(F.col(\"file_path\")).alias(\"num_contigs\"),\n",
    "    F.min(F.col(\"length\")).alias(\"min_length\"),\n",
    "    F.median(F.col(\"length\")).cast(\"int\").alias(\"med_length\"),\n",
    "    F.max(F.col(\"length\")).alias(\"max_length\")\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f37805",
   "metadata": {
    "id": "36f37805"
   },
   "source": [
    "### Activity 2-2: Parsing FASTA & GFF files using Axolotl.IO\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e4c20",
   "metadata": {
    "id": "744e4c20"
   },
   "outputs": [],
   "source": [
    "from axolotl.io.fasta import FastaIO\n",
    "\n",
    "# just like previously\n",
    "input_fasta_paths = [path.join(folder_path, \"*/*.fna\") for folder_path in generate_folder_paths(included_genera)]\n",
    "\n",
    "# this time, we use FastaIO class to load, which wraps around the original wholeTextFiles() function\n",
    "contigs = FastaIO.loadSmallFiles(\n",
    "    \",\".join(input_fasta_paths),\n",
    "    seq_type = \"nucl\",\n",
    "    minPartitions = MIN_PARTITIONS\n",
    ")\n",
    "%time print(contigs.df.persist().count())\n",
    "contigs.df.createOrReplaceTempView(\"contigs\")\n",
    "\n",
    "contigs.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e12b2",
   "metadata": {
    "id": "3d1e12b2"
   },
   "outputs": [],
   "source": [
    "from axolotl.io.gff3 import gff3IO\n",
    "\n",
    "# now we change the .fna into .gff\n",
    "input_gff_paths = [path.join(folder_path, \"*/*.gff\") for folder_path in generate_folder_paths(included_genera)]\n",
    "\n",
    "features = gff3IO.loadSmallFiles(\n",
    "    \",\".join(input_gff_paths),\n",
    "    fasta_path_func = lambda x: x.rsplit(\".\", 1)[0].rsplit(\"_sanntis\", 1)[0] + \".fna\",\n",
    "    minPartitions = MIN_PARTITIONS\n",
    ")\n",
    "%time print(features.df.persist().count())\n",
    "features.df.createOrReplaceTempView(\"features\")\n",
    "\n",
    "features.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e88c9",
   "metadata": {
    "id": "646e88c9"
   },
   "outputs": [],
   "source": [
    "# let's get some statistics for the parsed GFFs\n",
    "\n",
    "spark.sql((\n",
    "    \"SELECT type, count(type) as total_rows from features\"\n",
    "    \" GROUP BY type\"\n",
    "    \" ORDER BY total_rows DESC\"\n",
    ")).toPandas().set_index(\"type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab08d8",
   "metadata": {
    "id": "55ab08d8"
   },
   "source": [
    "### Exercise: Scaling Up Data without Changing the Code\n",
    "\n",
    "Let's select these new genera below, totaling to around 38 genomes (~4x larger), for parsing. **Using Axolotl's fastaIO and gff3IO, can you parse those genomes into new DataFrames?**\n",
    "\n",
    "Phylum - Genus\n",
    "\n",
    "- Actinomycetota - Microbacterium (13 genomes)\n",
    "- Bacteroidota - Aquimarina (10 genomes)\n",
    "- Bacillota - Bacillus (9 genomes)\n",
    "- Pseudomonadota - Kordiimonas (6 genomes)\n",
    "\n",
    "\n",
    "**_Assign new variable and table names for the new DataFrames_ --> use contigs_2 for the FASTA files and features_2 for the GFF files, as they will be used in the follow-up blocks**. Hint: use the same code as previously (check boxes before this), but reassign the variables and the input array.\n",
    "\n",
    "Put your code down here (remove the comment tags and fill in the blank '...' with the correct code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WfFYvcXwlse",
   "metadata": {
    "id": "6WfFYvcXwlse"
   },
   "outputs": [],
   "source": [
    "#included_genera_2 = [\n",
    "# ...\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AP4x3YtYjYuE",
   "metadata": {
    "id": "AP4x3YtYjYuE"
   },
   "outputs": [],
   "source": [
    "#input_fasta_paths_2 = ...\n",
    "\n",
    "#contigs_2 = ...\n",
    "\n",
    "#%time print(contigs_2.df.persist().count())\n",
    "#contigs.df.createOrReplaceTempView(\"contigs_2\")\n",
    "\n",
    "#contigs.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wgvMKUUGj6qK",
   "metadata": {
    "id": "wgvMKUUGj6qK"
   },
   "outputs": [],
   "source": [
    "#input_gff_paths_2 = ...\n",
    "\n",
    "#features_2 = ...\n",
    "\n",
    "#%time print(features_2.df.persist().count())\n",
    "#features.df.createOrReplaceTempView(\"features_2\")\n",
    "\n",
    "#features.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_dspsgZiRzW",
   "metadata": {
    "cellView": "form",
    "id": "P_dspsgZiRzW"
   },
   "outputs": [],
   "source": [
    "# @title spoiler code: uncomment the code below this box and run it if you are stuck\n",
    "\n",
    "def run_spoiler_code():\n",
    "  # this part redefine the input paths, the only thing needs to be changed from the original code\n",
    "  included_genera_2 = [\n",
    "      (\"p__Actinomycetota\", \"g__Microbacterium\"),\n",
    "      (\"p__Bacteroidota\", \"g__Aquimarina\"),\n",
    "      (\"p__Bacillota\", \"g__Bacillus\"),\n",
    "      (\"p__Pseudomonadota\", \"g__Kordiimonas\")\n",
    "  ]\n",
    "  # this part for the FASTAs\n",
    "  input_fasta_paths_2 = [path.join(folder_path, \"*/*.fna\") for folder_path in generate_folder_paths(included_genera_2)]\n",
    "  contigs_2 = FastaIO.loadSmallFiles(\n",
    "      \",\".join(input_fasta_paths_2),\n",
    "      seq_type = \"nucl\",\n",
    "      minPartitions = MIN_PARTITIONS\n",
    "  )\n",
    "  # this part for the GFFs\n",
    "  input_gff_paths_2 = [path.join(folder_path, \"*/*.gff\") for folder_path in generate_folder_paths(included_genera_2)]\n",
    "  features_2 = gff3IO.loadSmallFiles(\n",
    "      \",\".join(input_gff_paths_2),\n",
    "      fasta_path_func = lambda x: x.rsplit(\".\", 1)[0].rsplit(\"_sanntis\", 1)[0] + \".fna\",\n",
    "      minPartitions = MIN_PARTITIONS\n",
    "  )\n",
    "  return contigs_2, features_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XjclC6Zp1Njj",
   "metadata": {
    "id": "XjclC6Zp1Njj"
   },
   "outputs": [],
   "source": [
    "# uncomment and run this if you are stuck\n",
    "contigs_2, features_2 = run_spoiler_code()\n",
    "%time print(contigs_2.df.persist().count())\n",
    "%time print(features_2.df.persist().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971afa87",
   "metadata": {
    "id": "971afa87"
   },
   "source": [
    "### Combining two DataFrames using .union()\n",
    "\n",
    "Spark allows combining DataFrame (must be of the same structure) using DataFrame.union() function (see https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html). Axolotl support the same operations in its DataFrames, **however, note that this will reset the indexes in the combined AxlDF**.\n",
    "\n",
    "Below, we will **combine both the contig's and the feature's dataframes, then _persist them_**, so we can work on the combined version from this point forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecd12c",
   "metadata": {
    "id": "7cecd12c"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.sequence import NuclSeqDF\n",
    "\n",
    "contigs_combined = NuclSeqDF(contigs.df.union(contigs_2.df), override_idx=True)\n",
    "contigs_combined.df.persist()\n",
    "contigs_combined.df.count()\n",
    "contigs_combined.df.createOrReplaceTempView(\"contigs_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ca787",
   "metadata": {
    "id": "8b9ca787"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.annotation import RawFeatDF\n",
    "\n",
    "features_combined = RawFeatDF(features.df.union(features_2.df), override_idx=True)\n",
    "features_combined.df.persist()\n",
    "features_combined.df.count()\n",
    "features_combined.df.createOrReplaceTempView(\"features_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dac92",
   "metadata": {
    "id": "9f8dac92"
   },
   "source": [
    "#### Let's generate some visualization for the combined data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76s4TvbVAXIv",
   "metadata": {
    "id": "76s4TvbVAXIv"
   },
   "source": [
    "<i>Map contig lengths per genus</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361a00e",
   "metadata": {
    "id": "4361a00e"
   },
   "outputs": [],
   "source": [
    "# map out contig lengths per genus\n",
    "ctg_length_summary = spark.sql((\n",
    "    \"SELECT printf('%s:%s ', Phylum, Genus) as genus, collect_list(log(length)) as ctg_lengths\" # collect contig lengths per genus as a list\n",
    "    \" FROM contigs_combined JOIN taxonomy ON split(seq_id, '_')[0]=taxonomy.Genome\" # join with taxonomy table using genome IDs\n",
    "    \" GROUP BY Phylum, Genus\"\n",
    "    \" ORDER BY Phylum, Genus ASC\"\n",
    ")).toPandas()\n",
    "ctg_length_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L7wm9VeoAjRh",
   "metadata": {
    "id": "L7wm9VeoAjRh"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4));\n",
    "plt.title(\"Contig lengths (log10)\");\n",
    "plt.xticks(rotation=-30, ha=\"left\");\n",
    "plt.boxplot(ctg_length_summary[\"ctg_lengths\"].values, labels=ctg_length_summary[\"genus\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89805358",
   "metadata": {
    "id": "89805358"
   },
   "outputs": [],
   "source": [
    "# map out BGC numbers per genome per genus\n",
    "bgc_num_summary = spark.sql((\n",
    "    \"SELECT genus, collect_list(num_bgcs) as num_bgcs\"\n",
    "    \" FROM (SELECT printf('%s:%s ', Phylum, Genus) as genus, count(type) as num_bgcs\"\n",
    "          \" FROM features_combined JOIN taxonomy ON split(seq_id, '_')[0]=taxonomy.Genome\"\n",
    "          \" WHERE type like 'CLUSTER'\"\n",
    "          \" GROUP BY taxonomy.Genome, Phylum, Genus)\"\n",
    "    \" GROUP BY genus\"\n",
    "    \" ORDER BY genus ASC\"\n",
    ")).toPandas()\n",
    "\n",
    "bgc_num_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wXUjl-gPK8y4",
   "metadata": {
    "id": "wXUjl-gPK8y4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4));\n",
    "#plt.title(\"Contig lengths (log10)\");\n",
    "plt.xticks(rotation=-30, ha=\"left\");\n",
    "plt.boxplot(bgc_num_summary[\"num_bgcs\"].values, labels=bgc_num_summary[\"genus\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953c9c1",
   "metadata": {
    "id": "7953c9c1"
   },
   "source": [
    "### Activity 2-3: Extracting BGC and CDS tables from Raw Features table\n",
    "\n",
    "A common data preprocessing activity is transforming (through filtering, joining, function calling) one or more data tables into another. In this case, we take an example of transforming the RawFeatDF (features table) parsed directly from the GFF files into a separate cdsDF (which contain only CDS features data) and bgcDF (which contain only BGC features data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tMAtDnUQNGVA",
   "metadata": {
    "id": "tMAtDnUQNGVA"
   },
   "source": [
    "<i>As a reference, the original RawFeatDF table looks like this:</i> (most of the important information is stored in the 'qualifiers' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PyANamu_NFLr",
   "metadata": {
    "id": "PyANamu_NFLr"
   },
   "outputs": [],
   "source": [
    "features_combined.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yKPP4MmUNXHe",
   "metadata": {
    "id": "yKPP4MmUNXHe"
   },
   "source": [
    "#### RawFeatDF -> cdsDF\n",
    "\n",
    "<b>cdsDF</b>, as the name suggests, is an Axolotl dataframe class specifically made for storing Coding Sequences (CDS) information. In a typical genome annotation scenario (using GFF or GBK files), CDS information is stored as features, with type == 'CDS', and include information such as locus tag, gene id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DQi7TwO3N5hk",
   "metadata": {
    "id": "DQi7TwO3N5hk"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.annotation import cdsDF\n",
    "\n",
    "# to 'extract' cdsDF from a RawFeatDF table, we use the fromRawFeatDF() function\n",
    "cds_combined = cdsDF.fromRawFeatDF(features_combined)\n",
    "%time print(cds_combined.df.persist().count()) # persist this for further analysis\n",
    "\n",
    "# this is what cdsDF table will looks like:\n",
    "# the 'locus_tag', 'gene_name', etc. were derived from the original 'qualifiers' column of the RawFeatDF\n",
    "cds_combined.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gvX6l8nSahS",
   "metadata": {
    "id": "_gvX6l8nSahS"
   },
   "source": [
    "#### utilizing Axolotl's pre-made functions\n",
    "\n",
    "Notice how in the table above, the 'aa_sequence' column seems to be empty. While some annotation pipeline store AA sequence in the feature qualifiers, some don't. In this case, we will need to re-translate based on the location of the CDS and the nucleotide sequences (i.e., codons) taken from the contigs.\n",
    "\n",
    "In Axolotl, the cdsDF class support this function via the <b>cdsDF.translateAAs()</b> function. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IpS8ci5tSZX_",
   "metadata": {
    "id": "IpS8ci5tSZX_"
   },
   "outputs": [],
   "source": [
    "# see how many CDS rows are untranslated\n",
    "cds_combined.df.select(F.isnotnull(F.col(\"aa_sequence\")).alias(\"have_translation\")).groupBy(\"have_translation\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9VxfoGxgVbpz",
   "metadata": {
    "id": "9VxfoGxgVbpz"
   },
   "outputs": [],
   "source": [
    "# let's use Axolotl to translate the AAs. Since the GFF also doesn't provide transl_table value, we will use the most common table = 11\n",
    "# this will take around 1.5 minutes\n",
    "\n",
    "cds_combined_translated = cds_combined.translateAAs(contigs_combined, default_table=11) # assign into a new variable\n",
    "%time cds_combined_translated.df.persist().count() # to kick start the process, we will persist and collect the new DF\n",
    "\n",
    "cds_combined_translated.df.filter(F.isnotnull(F.col(\"aa_sequence\"))).limit(3).toPandas() # let's see how the new table looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AiWS3FV9WEWM",
   "metadata": {
    "id": "AiWS3FV9WEWM"
   },
   "outputs": [],
   "source": [
    "# see how many CDS rows are now translated\n",
    "cds_combined_translated.df.select(F.isnotnull(F.col(\"aa_sequence\")).alias(\"have_translation\")).groupBy(\"have_translation\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hG4QfHIve_q2",
   "metadata": {
    "id": "hG4QfHIve_q2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 6));\n",
    "plt.title(\"CDS lengths\");\n",
    "plt.boxplot(cds_combined_translated.df.select(F.length(F.col(\"aa_sequence\")).alias(\"length\")).toPandas()[\"length\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9FaibOJMWx1N",
   "metadata": {
    "id": "9FaibOJMWx1N"
   },
   "source": [
    "### utilizing Axolotl to perform parallel HMM scanning (using the pyHMMer library)\n",
    "\n",
    "While Axolotl doesn't yet have an extensive list of pre-made functions and pipelines to cover the breadth of analyses people may do in genomics, we implement some most common ones, partly in order to demonstrate how flexible the library is for incorporating external tools, libraries and softwares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-TE-ZiiRjFlH",
   "metadata": {
    "id": "-TE-ZiiRjFlH"
   },
   "source": [
    "<i>due to time constraints, we will only run the scanning on a limited subset of the cdsDF table</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34699d",
   "metadata": {
    "id": "4c34699d"
   },
   "outputs": [],
   "source": [
    "from axolotl.app.bgc_analysis.functions import scan_cdsDF\n",
    "\n",
    "cds_example_limited = cdsDF(cds_combined_translated.df.limit(500).repartition(500), keep_idx=True)\n",
    "\n",
    "hmm_result = scan_cdsDF(cds_example_limited, \"Pfam-A.hmm\")\n",
    "\n",
    "%time hmm_result.persist().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y9n9n1AdnQ3c",
   "metadata": {
    "id": "Y9n9n1AdnQ3c"
   },
   "source": [
    "This is what the resulting hits table will looks like. The information provided will be enough to generate a multi-alignment table for all the sequences against the profile HMM (based on locations + gaps and the original CDS sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sXxyIbKyxiqC",
   "metadata": {
    "id": "sXxyIbKyxiqC"
   },
   "outputs": [],
   "source": [
    "hmm_result.select(\"cds_id\", \"hmm_name\").groupBy(\"hmm_name\").count().orderBy(\"count\", ascending=False).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B_6i716Png97",
   "metadata": {
    "id": "B_6i716Png97"
   },
   "source": [
    "#### an extra example, showing the bgcDF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6a112",
   "metadata": {
    "id": "34f6a112"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.annotation import bgcDF\n",
    "\n",
    "bgc_combined = bgcDF.fromRawFeatDF(\n",
    "    features_combined,\n",
    "    source_type=\"custom\",\n",
    "    kw_type=\"CLUSTER\",\n",
    "    kw_classes=\"nearest_MiBIG_class\"\n",
    ")\n",
    "%time print(bgc_combined.df.persist().count())\n",
    "\n",
    "bgc_combined.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee680e7",
   "metadata": {
    "id": "2ee680e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PnFjEaxzmmxs",
   "metadata": {
    "id": "PnFjEaxzmmxs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515c9dde",
   "metadata": {
    "id": "515c9dde"
   },
   "source": [
    "#### cluster-specific setup, adjust this according to your own ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rzO--dOLL_dQ",
   "metadata": {
    "id": "rzO--dOLL_dQ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m pip install pyspark matplotlib pandas\n",
    "!python -m pip install git+https://github.com/JGI-Bioinformatics/axolotl.git@16a87b15e4e4269bc871d7f010a1c492702cb0fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b21bac4",
   "metadata": {
    "id": "3b21bac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-23 16:56:34--  https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.3.91, 192.178.50.91, 142.250.217.219, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.3.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40554863 (39M) [application/java-archive]\n",
      "Saving to: ‘gcs-connector-hadoop2-latest.jar.3’\n",
      "\n",
      "gcs-connector-hadoo 100%[===================>]  38.68M  9.68MB/s    in 4.3s    \n",
      "\n",
      "2024-09-23 16:56:39 (9.06 MB/s) - ‘gcs-connector-hadoop2-latest.jar.3’ saved [40554863/40554863]\n",
      "\n",
      "24/09/23 16:56:40 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.0.179 instead (on interface wlp0s20f3)\n",
      "24/09/23 16:56:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/09/23 16:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    " #start a new Spark session, necessary for all Spark-related operations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# establish read-only access to the HDFS data for the workshop\n",
    "!wget https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar\n",
    "private_key_id = \"dce328180ba3fe571950393bfe0f8b8636065627\"\n",
    "private_key = \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQC7CQxPccIqGCJY\\niNLAPyOFYFMfnaF68LazCzvT0nInbJmRJFLO9od/pCdKoWzO3QqYGg64jmrzQEET\\ngWFoQMuuYkN7+jNRFwevH9g4uq4H7EmiJOYjRx/VruhuWPrQfNlilozCBSxc7aqU\\nuW12A6hUJzW+qIAhB7BIPm3DDW8Q+oa5nrKrwDUf9VqiHYjr6koU6HoGmwN5Log8\\n0il0HKZ8TXwyiV5jNcTr7CL0V/zocEAsLd1iqz4fXipVRxqqiTO68LOJyOxvkvfA\\nS7hjeBAtPR0qVxiCG8XbehEAI9P/mnuREsf1pjzdh4L4W5AVxh/dmynic/fCfzlB\\neiPGR7uDAgMBAAECggEAFKMuR+MmKNLk50lMR94vozCPu3X/MruLd+djaz9V9+JU\\ncjQZ1wWoydK+ayOzXS1SqpnQH1kc6p5KZUaUuEDTY03VuGumbJTSn5DZ9i3TChjD\\nvy+8cmQPWoVO1u4xLqliqy2FdoXE8V61axeoFlTXpqwfkrFbrJ6r5YIjRlnIdDeg\\nF8/T5OX6KM2xITBstVDUFv5qiFzJ000+cGGxT7p0E4S4rNsp43sDy2HQhBNasH8U\\nUFdl7Iu5VQxmvzi0+bwev5BP3rv8dC7uzG8Ge6kIBtBcPXUoj1gaiM51zpwqoHxz\\nzSe7y2NVr6PVkY10csachLa8Uw8qiJ5w6Rhqd5xAIQKBgQDqdjAhhEaehfDZI2nO\\nXXjjJsGZwQS4ZqN1yfrm0jYGLEjFYGbBnvPmxc8ZNzjzF8pcdqZeSC+Ux8dZ7xyp\\nE9GF6OH8iZwO8DE5OLCLiMg3Pk7Dtd9V3ud14htsDEHF/xqHIuZkNZbs8f5buiyw\\na8o5xJL0a+JygSES4M8nmbKUSQKBgQDMN4owygsNDQ3arO24UtHVS3nTpC4otMks\\n6ydxmGXjw+UI5X5k1UPmRvTNTO08nBK7cVUla/FLDWutvhlNapeQhPBoWUYrroLY\\nZS20dwN0BVylkxl/T0eBGk4aF/5VBJF04YEbcbw+8KaKe7LBinyAhPZUaUySpQwn\\nMkrNiaO5awKBgEqrqViDUBpcfgApjyDyE/YD+dSF7ILt7VEuKoIooJwGroZt1UwE\\n25i7luFo/PIuuXwJLaMJSl4P4iV6SZ5veWM0cASFrvXy+TpIG7HUti8h4OPjoGUw\\nwHaCtEkM/kWYoVI4gzeW+aIsz91WxbTKH3WWbVb6pPvitC3W+yKZiI5BAoGASScM\\nBXYah05JzC9t6D0ilTk4JqwFLOe1uLVzp6ljtjcR8CHlKNGMRQzd3DzWLoPp4eMz\\nEPidsrjX/aC/B5BLQqWcSITSMguNK17zBFYtkn3pNhZ/Z69KeeCmYYp+vI2qXSf6\\nS45uVE14Gts8qT+mYZM/efVOTKl6tP+68+1+DIkCgYAyAhU7yfejk/Oa/0VWxDKX\\nkV3DcjIdr6db33IhnERKeqnVoh6psZ/JwfwjkjgU6fTYfZBwfbVPx5QNE4ZLikyl\\n7Fv6w0ubsem905BY2drE4eb2X3Rrbnpxe995x4gwtMO/tnEi792MGyYRpwzM+Rb2\\n0YWsVUcSEWqwvwfR25YKLA==\\n-----END PRIVATE KEY-----\\n\"\n",
    "\n",
    "# 'spark' and 'sc' are the canonical variable names for the SparkSession and SparkContext objects\n",
    "spark = SparkSession.builder\\\n",
    "    .config(\"spark.jars\", \"./gcs-connector-hadoop2-latest.jar\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.enable\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.email \", \"jgi-ga-readonly@jgi-ga.iam.gserviceaccount.com\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.project.id\", \"jgi-ga\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.private.key\", f\"{private_key}\")\\\n",
    "    .config(\"spark.hadoop.fs.gs.auth.service.account.private.key.id\", f\"{private_key_id}\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd9d49a",
   "metadata": {
    "id": "5cd9d49a"
   },
   "outputs": [],
   "source": [
    "# point here to where the workshop's data folder is located\n",
    "from os import path\n",
    "\n",
    "DATA_FOLDER = \"gs://zw_axolotl/jgi_workshop_2024/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afa8e1ae",
   "metadata": {
    "id": "afa8e1ae"
   },
   "outputs": [],
   "source": [
    "# point here to where we will store results from the activity (make sure it's empty and readable!)\n",
    "\n",
    "RESULT_FOLDER = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d37f304",
   "metadata": {
    "id": "0d37f304"
   },
   "outputs": [],
   "source": [
    "# helper variable to set minimum repartition of the data (to use up all executors)\n",
    "\n",
    "MIN_PARTITIONS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "TtNU0Y_c2Lex",
   "metadata": {
    "id": "TtNU0Y_c2Lex"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# to avoid needing to mount a Gdrive folder, we will copy an existing parquet for the taxonomy table from module 1\n",
    "spark.read.parquet(path.join(DATA_FOLDER, \"df_taxonomy.pq\")).write.mode(\"overwrite\").parquet(path.join(RESULT_FOLDER, \"df_taxonomy.pq\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1843403",
   "metadata": {
    "id": "f1843403"
   },
   "source": [
    "#### variables setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6b3546a",
   "metadata": {
    "id": "c6b3546a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F # contain frequently-used functions\n",
    "from pyspark.sql import types as T # for specifying schema datatypes, etc\n",
    "\n",
    "# basic python libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa21d9b5",
   "metadata": {
    "id": "aa21d9b5"
   },
   "source": [
    "### The dataset\n",
    "The full dataset comprises 13,223 \"Species representative\" genomes of the MGnify Marine V2.0 catalogue (https://www.ebi.ac.uk/metagenomics/genome-catalogues/marine-v2-0).\n",
    "\n",
    "Each genome folder contain:\n",
    "- Fasta files of the contig sequences: *.fna\n",
    "- GFF3 files of gene-calling annotations: *.gff\n",
    "- GFF3 files of BGC-calling annotations (using their in-house pipeline): *_sanntis.gff\n",
    "\n",
    "The data folder \"genomes_all\" is organized by Phylum->Genus->Genome, for example:\n",
    "\n",
    "- genomes_all\n",
    " - Actinomycetota\n",
    "   - Streptomycetes\n",
    "     - MYM000001\n",
    "       - MYM000001.fna\n",
    "       - MYM000001.gff\n",
    "       - MYM000001_sanntis.gff\n",
    "   - Microbacteria\n",
    "   - ...\n",
    " - Pseudomonatota\n",
    "   - ...\n",
    " - ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f42908",
   "metadata": {
    "id": "e8f42908"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13223"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the metadata (and persist it) for the entire dataset, as we will be using it later on\n",
    "\n",
    "metadata_sdf = (spark.read\n",
    "    .option(\"delimiter\", \"\\t\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"samplingRatio\", 0.0001)\n",
    "    .csv(path.join(DATA_FOLDER, \"mgnify-marine-v2-0.tsv\"))\n",
    ").filter(F.col(\"Genome\") == F.col(\"Species_rep\")) # takes only the species representative genomes from the full 5X,XXX genomes\n",
    "\n",
    "metadata_sdf.createOrReplaceTempView(\"metadata\")\n",
    "metadata_sdf.persist()\n",
    "metadata_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "X-Qx37tWaBK9",
   "metadata": {
    "id": "X-Qx37tWaBK9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13223"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxonomy_pq_path = path.join(RESULT_FOLDER, \"df_taxonomy.pq\") # from module_1\n",
    "\n",
    "# we will load the taxonomy as SQL, after filtering it only for the 13,223 species rep genomes (via Join)\n",
    "spark.read.parquet(taxonomy_pq_path).createOrReplaceTempView(\"taxonomy\")\n",
    "taxonomy_sdf = spark.sql(\"SELECT taxonomy.* FROM taxonomy JOIN metadata ON taxonomy.Genome=metadata.Genome\")\n",
    "taxonomy_sdf.createOrReplaceTempView(\"taxonomy\")\n",
    "\n",
    "taxonomy_sdf.persist()\n",
    "taxonomy_sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "_u9Y8E4-irJ-",
   "metadata": {
    "id": "_u9Y8E4-irJ-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phylum</th>\n",
       "      <th>Genus</th>\n",
       "      <th>num_genomes</th>\n",
       "      <th>num_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p__Pseudomonadota</td>\n",
       "      <td>g__Pelagibacter</td>\n",
       "      <td>830</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p__Pseudomonadota</td>\n",
       "      <td>g__</td>\n",
       "      <td>348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p__Cyanobacteriota</td>\n",
       "      <td>g__Prochlorococcus_A</td>\n",
       "      <td>214</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p__Pseudomonadota</td>\n",
       "      <td>g__Pelagibacter_A</td>\n",
       "      <td>151</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p__Bacteroidota</td>\n",
       "      <td>g__</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p__Thermoplasmatota</td>\n",
       "      <td>g__MGIIa-L1</td>\n",
       "      <td>102</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p__Pseudomonadota</td>\n",
       "      <td>g__AG-337-I02</td>\n",
       "      <td>100</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p__Planctomycetota</td>\n",
       "      <td>g__</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p__Pseudomonadota</td>\n",
       "      <td>g__Vibrio</td>\n",
       "      <td>76</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p__Pseudomonadota</td>\n",
       "      <td>g__UBA9145</td>\n",
       "      <td>75</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Phylum                 Genus  num_genomes  num_species\n",
       "0    p__Pseudomonadota       g__Pelagibacter          830          719\n",
       "1    p__Pseudomonadota                   g__          348            1\n",
       "2   p__Cyanobacteriota  g__Prochlorococcus_A          214          199\n",
       "3    p__Pseudomonadota     g__Pelagibacter_A          151          118\n",
       "4      p__Bacteroidota                   g__          149            1\n",
       "5  p__Thermoplasmatota           g__MGIIa-L1          102           49\n",
       "6    p__Pseudomonadota         g__AG-337-I02          100           89\n",
       "7   p__Planctomycetota                   g__           78            1\n",
       "8    p__Pseudomonadota             g__Vibrio           76           72\n",
       "9    p__Pseudomonadota            g__UBA9145           75           40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will create a new table, to lookup the number of genomes + species included in a single genus\n",
    "genus_stat_sdf = spark.sql((\n",
    "    \"SELECT Phylum, Genus, count(Genome) as num_genomes, count(distinct Species) as num_species\"\n",
    "    \" FROM taxonomy\"\n",
    "    \" GROUP BY Phylum, Genus\"\n",
    "))\n",
    "\n",
    "genus_stat_sdf.createOrReplaceTempView(\"genus_stat\")\n",
    "genus_stat_sdf.persist().count()\n",
    "\n",
    "# here is what the table looks like\n",
    "genus_stat_sdf.orderBy(\"num_genomes\", ascending=False).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Lbzxnvfj_2G",
   "metadata": {
    "id": "-Lbzxnvfj_2G"
   },
   "source": [
    "#### For this activity, let's look up a total of <10 genomes from two different genera, all with unique species\n",
    "\n",
    "In this case, we will arbitarily choose <i>Streptomyces</i> and <i>Kitasatospora</i>, two related genera that were known for their biosynthetic potentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "AIEvrhhuj-yW",
   "metadata": {
    "id": "AIEvrhhuj-yW"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phylum</th>\n",
       "      <th>Genus</th>\n",
       "      <th>num_genomes</th>\n",
       "      <th>num_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p__Actinomycetota</td>\n",
       "      <td>g__Streptomyces</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p__Bacillota</td>\n",
       "      <td>g__Streptococcus</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p__Actinomycetota</td>\n",
       "      <td>g__Kitasatospora</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Phylum             Genus  num_genomes  num_species\n",
       "0  p__Actinomycetota   g__Streptomyces            7            7\n",
       "1       p__Bacillota  g__Streptococcus            1            1\n",
       "2  p__Actinomycetota  g__Kitasatospora            1            1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we will look up the correct naming of our genera of choice, along with their genomes/species count\n",
    "# we can use the wild card character '%' to search using the SQL query\n",
    "\n",
    "spark.sql((\n",
    "    \"SELECT * FROM genus_stat\"\n",
    "    \" WHERE Genus LIKE '%Strepto%' OR Genus LIKE '%Kita%'\"\n",
    ")).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "Opg7Qa-8hBVT",
   "metadata": {
    "id": "Opg7Qa-8hBVT"
   },
   "outputs": [],
   "source": [
    "# then let's encode and specify the genera to be selected, complete with its Phylum\n",
    "included_genera = [\n",
    "    (\"p__Actinomycetota\", \"g__Streptomyces\"),\n",
    "    (\"p__Actinomycetota\", \"g__Kitasatospora\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "qXq88xAIcJ-b",
   "metadata": {
    "id": "qXq88xAIcJ-b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://zw_axolotl/jgi_workshop_2024/data/genomes_all/Actinomycetota/Streptomyces',\n",
       " 'gs://zw_axolotl/jgi_workshop_2024/data/genomes_all/Actinomycetota/Kitasatospora']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the function that will later on be used to generate\n",
    "# the list of folder paths of each genus\n",
    "# all unknown genera ('g__') is combined into a folder called 'unknown_genus'\n",
    "\n",
    "def generate_folder_paths(list_of_genera, file_extension=None):\n",
    "    return [path.join(DATA_FOLDER, \"genomes_all\", phylum.split(\"__\")[1], genus.split(\"__\")[1] if genus != \"g__\" else \"unknown_genus\") for phylum, genus in list_of_genera]\n",
    "\n",
    "# for example, here is the folder we selected\n",
    "generate_folder_paths(included_genera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81dcae",
   "metadata": {
    "id": "6e81dcae"
   },
   "source": [
    "### Activity 2-1: Parsing FASTA files from scratch with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65846124",
   "metadata": {
    "id": "65846124"
   },
   "source": [
    "First, let's make a FASTA parser function that takes: a **file path string** and a **nucleotide FASTA text (can be multi-sequences)** and output list of sequences and its IDs+headers, along with its original file path, as follow:\n",
    "\n",
    "**Input:** ('example_fasta.fna',\n",
    "'\n",
    "&gt;seq_1 desc_1\n",
    "<br/>ATGCATGCATGC<br/>ATGCATGCATGC\n",
    "\n",
    "&gt;seq_2 desc_2\n",
    "<br/>AAAAAAAAAAAA\n",
    "<br/>TTTTTTTTTTTT\n",
    "'\n",
    "\n",
    "**Output:**\n",
    "\n",
    "[<br/>\n",
    "['example_fasta.fna', 'seq_1', 'desc_1', 'ATGCATGCATGCATGCATGCATGC'],<br/>\n",
    "['example_fasta.fna', 'seq_2', 'desc_2', 'AAAAAAAAAAAATTTTTTTTTTTT']\n",
    "<br/>]\n",
    "\n",
    "This function will be called later in combination with Spark's parallel file parsing function 'wholeTextFiles()' to produce a Spark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ac78d31",
   "metadata": {
    "id": "5ac78d31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 103 µs, sys: 65 µs, total: 168 µs\n",
      "Wall time: 173 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['example.fasta', 'seq1', 'Description of seq1', 'ATGCGTA'],\n",
       " ['example.fasta', 'seq2', 'Another description', 'GCTAGCTAGCTA'],\n",
       " ['example.fasta', 'seq3', 'Yet another description', 'TGCATGCA']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this code is derived from chatGPT\n",
    "\n",
    "import re\n",
    "\n",
    "def parse_fasta(file_path, fasta_str):\n",
    "    sequences = []\n",
    "    fasta_entries = fasta_str.strip().split('>')\n",
    "\n",
    "    for entry in fasta_entries:\n",
    "        if entry:\n",
    "            lines = entry.split('\\n')\n",
    "            header = lines[0]\n",
    "            sequence = ''.join(lines[1:])\n",
    "\n",
    "            match = re.match(r'(\\S+)\\s*(.*)', header)\n",
    "            if match:\n",
    "                seq_id = match.group(1)\n",
    "                description = match.group(2)\n",
    "                sequences.append([file_path, seq_id, description, sequence])\n",
    "\n",
    "    return sequences\n",
    "\n",
    "# test\n",
    "file_path = \"example.fasta\"\n",
    "fasta_str = \"\"\">seq1 Description of seq1\n",
    "ATGCGTA\n",
    ">seq2 Another description\n",
    "GCTAGCTAGCTA\n",
    ">seq3 Yet another description\n",
    "TGCATGCA\"\"\"\n",
    "\n",
    "%time parse_fasta(file_path, fasta_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d7eb2",
   "metadata": {
    "id": "564d7eb2"
   },
   "source": [
    "#### Parsing using Spark's wholeTextFiles()\n",
    "\n",
    "with Spark, we can use the wholeTextFiles() to scan and load text files into RDDs of string,\n",
    "then pass it down to our parser function. This works well for smaller files that can fit each\n",
    "into the worker's memory. See https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.wholeTextFiles.html\n",
    "\n",
    "The function takes file paths as a single string, separated by <b>','</b>. You can use glob-style wildcard <b>'*'</b> to capture multiple files with the same pattern.\n",
    "\n",
    "If you are parsing fewer, but big files, you would use https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.textFile.html instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "OZ3KKt2kml69",
   "metadata": {
    "id": "OZ3KKt2kml69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://zw_axolotl/jgi_workshop_2024/data/genomes_all/Actinomycetota/Streptomyces/*/*.fna',\n",
       " 'gs://zw_axolotl/jgi_workshop_2024/data/genomes_all/Actinomycetota/Kitasatospora/*/*.fna']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's generate the input fasta paths (using the function we wrote above, combining it with the included genera)\n",
    "input_fasta_paths = [path.join(folder_path, \"*/*.fna\") for folder_path in generate_folder_paths(included_genera)]\n",
    "input_fasta_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c229dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    545\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 546\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_walk_global_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwholeTextFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://zw_axolotl/jgi_workshop_2024/data/genomes_all/Actinomycetota/Streptomyces/*/*.fna\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3503\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3505\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   3511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   3512\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3361\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   3366\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3371\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   3343\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 3345\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "sc.wholeTextFiles(\"gs://zw_axolotl/jgi_workshop_2024/data/genomes_all/Actinomycetota/Streptomyces/*/*.fna\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f57c80d",
   "metadata": {
    "id": "0f57c80d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    545\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 546\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_walk_global_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# flatMap() is used to flatten the list of lists into single list (i.e., row in the dataframe)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m from_spark_df \u001b[38;5;241m=\u001b[39m (\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwholeTextFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fasta_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtup\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_fasta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# wholeTextFiles output: RDD of (file_path, file_content_string)\u001b[39;49;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdesc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlength(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \u001b[38;5;66;03m# add a new column: length\u001b[39;00m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      8\u001b[0m from_spark_df\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m      9\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfrom_spark_df.count()\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inferSchema\u001b[39m(\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    527\u001b[0m     rdd: RDD[Any],\n\u001b[1;32m    528\u001b[0m     samplingRatio: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    529\u001b[0m     names: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    530\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m StructType:\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;124;03m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 546\u001b[0m     first \u001b[38;5;241m=\u001b[39m \u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m first:\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe first row in RDD is empty, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan not infer schema\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1891\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[1;32m   1893\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[0;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3503\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3505\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   3511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   3512\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3361\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   3366\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3371\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   3343\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 3345\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# flatMap() is used to flatten the list of lists into single list (i.e., row in the dataframe)\n",
    "from_spark_df = (sc.wholeTextFiles(\",\".join(input_fasta_paths)).flatMap(\n",
    "    lambda tup: parse_fasta(tup[0], tup[1]) # wholeTextFiles output: RDD of (file_path, file_content_string)\n",
    ").toDF([\"file_path\", \"seq_id\", \"desc\", \"sequence\"])\n",
    "    .withColumn(\"length\", F.length(F.col(\"sequence\"))) # add a new column: length\n",
    ")\n",
    "\n",
    "from_spark_df.persist()\n",
    "%time from_spark_df.count()\n",
    "\n",
    "from_spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a27c4",
   "metadata": {
    "id": "bc9a27c4"
   },
   "outputs": [],
   "source": [
    "# let's check the statistics\n",
    "from_spark_df.select(\n",
    "    F.countDistinct(F.col(\"file_path\")).alias(\"num_files\"),\n",
    "    F.count(F.col(\"file_path\")).alias(\"num_contigs\"),\n",
    "    F.min(F.col(\"length\")).alias(\"min_length\"),\n",
    "    F.median(F.col(\"length\")).cast(\"int\").alias(\"med_length\"),\n",
    "    F.max(F.col(\"length\")).alias(\"max_length\")\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f37805",
   "metadata": {
    "id": "36f37805"
   },
   "source": [
    "### Activity 2-2: Parsing FASTA & GFF files using Axolotl.IO\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "744e4c20",
   "metadata": {
    "id": "744e4c20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 692, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 565, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 546, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 157, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in _extract_code_globals\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/satria/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py\", line 334, in <dictcomp>\n",
      "    out_names = {names[oparg]: None for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:692\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:565\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamic_function_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:546\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    545\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[0;32m--> 546\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43m_function_getstate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    548\u001b[0m         _function_setstate)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle_fast.py:157\u001b[0m, in \u001b[0;36m_function_getstate\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    146\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[1;32m    155\u001b[0m }\n\u001b[0;32m--> 157\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_code_globals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__code__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    159\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[0;34m(co)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moparg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_walk_global_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/cloudpickle/cloudpickle.py:334\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# We use a dict with None values instead of a set to get a\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# deterministic order (assuming Python 3.6+) and avoid introducing\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# non-deterministic pickle bytes as a results.\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {\u001b[43mnames\u001b[49m\u001b[43m[\u001b[49m\u001b[43moparg\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m input_fasta_paths \u001b[38;5;241m=\u001b[39m [path\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*/*.fna\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m folder_path \u001b[38;5;129;01min\u001b[39;00m generate_folder_paths(included_genera)]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# this time, we use FastaIO class to load, which wraps around the original wholeTextFiles() function\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m contigs \u001b[38;5;241m=\u001b[39m \u001b[43mFastaIO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadSmallFiles\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_fasta_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnucl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mminPartitions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMIN_PARTITIONS\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprint(contigs.df.persist().count())\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m contigs\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontigs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/axolotl/io/base.py:96\u001b[0m, in \u001b[0;36mAxlIO.loadSmallFiles\u001b[0;34m(cls, file_pattern, minPartitions, skip_malformed_record, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# fetch the number of nodes, and use for setting minPartitions number\u001b[39;00m\n\u001b[1;32m     91\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([executor\u001b[38;5;241m.\u001b[39mhost() \u001b[38;5;28;01mfor\u001b[39;00m executor \u001b[38;5;129;01min\u001b[39;00m spark\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mstatusTracker()\u001b[38;5;241m.\u001b[39mgetExecutorInfos()])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__postprocess(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_getOutputDFclass(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)(\n\u001b[1;32m     94\u001b[0m     \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwholeTextFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminPartitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminPartitions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mminPartitions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parseFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_malformed_record\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_malformed_record\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 96\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getOutputDFclass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m ), \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/sql/session.py:938\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 938\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_java_object_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    939\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), struct\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    940\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3113\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3110\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[1;32m   3111\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3503\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3505\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3510\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[1;32m   3511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[1;32m   3512\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3361\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[0;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[1;32m   3365\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[1;32m   3366\u001b[0m     env,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3371\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[1;32m   3372\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   3342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[1;32m   3343\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[1;32m   3344\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[0;32m-> 3345\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[1;32m   3348\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bigslice/lib/python3.11/site-packages/pyspark/serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[1;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from axolotl.io.fasta import FastaIO\n",
    "\n",
    "# just like previously\n",
    "input_fasta_paths = [path.join(folder_path, \"*/*.fna\") for folder_path in generate_folder_paths(included_genera)]\n",
    "\n",
    "# this time, we use FastaIO class to load, which wraps around the original wholeTextFiles() function\n",
    "contigs = FastaIO.loadSmallFiles(\n",
    "    \",\".join(input_fasta_paths),\n",
    "    seq_type = \"nucl\",\n",
    "    minPartitions = MIN_PARTITIONS\n",
    ")\n",
    "%time print(contigs.df.persist().count())\n",
    "contigs.df.createOrReplaceTempView(\"contigs\")\n",
    "\n",
    "contigs.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e12b2",
   "metadata": {
    "id": "3d1e12b2"
   },
   "outputs": [],
   "source": [
    "from axolotl.io.gff3 import gff3IO\n",
    "\n",
    "# now we change the .fna into .gff\n",
    "input_gff_paths = [path.join(folder_path, \"*/*.gff\") for folder_path in generate_folder_paths(included_genera)]\n",
    "\n",
    "features = gff3IO.loadSmallFiles(\n",
    "    \",\".join(input_gff_paths),\n",
    "    fasta_path_func = lambda x: x.rsplit(\".\", 1)[0].rsplit(\"_sanntis\", 1)[0] + \".fna\",\n",
    "    minPartitions = MIN_PARTITIONS\n",
    ")\n",
    "%time print(features.df.persist().count())\n",
    "features.df.createOrReplaceTempView(\"features\")\n",
    "\n",
    "features.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2be7b3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------------+--------------------+--------------------+----------+\n",
      "|       Genome|    Kingdom|              Phylum|               Genus|             Species|GC_content|\n",
      "+-------------+-----------+--------------------+--------------------+--------------------+----------+\n",
      "|MGYG000465641|d__Bacteria|  p__Gemmatimonadota|          g__UBA6934|s__UBA6934 sp0023...|     44.85|\n",
      "|MGYG000465642|d__Bacteria|     p__Bacteroidota|          g__MED-G14|s__MED-G14 sp0026...|     30.86|\n",
      "|MGYG000465643| d__Archaea| p__Thermoplasmatota|         g__MGIIb-N1|s__MGIIb-N1 sp002...|     47.56|\n",
      "|MGYG000465644|d__Bacteria|    p__Spirochaetota|          g__UBA3465|s__UBA3465 sp0027...|     52.98|\n",
      "|MGYG000465645|d__Bacteria|   p__Pseudomonadota|         g__UBA11869|s__UBA11869 sp002...|     35.24|\n",
      "|MGYG000465646|d__Bacteria|   p__Pseudomonadota|g__Pseudoalteromonas|s__Pseudoalteromo...|     40.79|\n",
      "|MGYG000465647|d__Bacteria|   p__Pseudomonadota|          g__UBA9145|s__UBA9145 sp0196...|     44.29|\n",
      "|MGYG000465648|d__Bacteria|   p__Pseudomonadota|     g__Marinobacter|s__Marinobacter n...|     57.54|\n",
      "|MGYG000465649|d__Bacteria|    p__Chloroflexota|      g__GCA-2712585|s__GCA-2712585 sp...|     43.11|\n",
      "|MGYG000465650|d__Bacteria|   p__Marinisomatota|    g__GCA-002701945|s__GCA-002701945 ...|     32.23|\n",
      "|MGYG000465651|d__Bacteria|   p__Pseudomonadota|      g__GCA-2720195|s__GCA-2720195 sp...|     33.26|\n",
      "|MGYG000465652|d__Bacteria|   p__Pseudomonadota| g__Pseudothioglobus|s__Pseudothioglob...|     38.34|\n",
      "|MGYG000465653|d__Bacteria|   p__Marinisomatota|    g__GCA-002701945|s__GCA-002701945 ...|      33.5|\n",
      "|MGYG000465654| d__Archaea| p__Thermoplasmatota|         g__MGIIb-O2|s__MGIIb-O2 sp002...|     38.05|\n",
      "|MGYG000465655|d__Bacteria|   p__Pseudomonadota|          g__TMED109|s__TMED109 sp0027...|     29.76|\n",
      "|MGYG000465656|d__Bacteria|   p__Pseudomonadota|      g__Alcanivorax|s__Alcanivorax pr...|     66.47|\n",
      "|MGYG000465657|d__Bacteria|    p__Chloroflexota|          g__TMED-70|s__TMED-70 sp0083...|     32.92|\n",
      "|MGYG000465658|d__Bacteria|   p__Pseudomonadota|                 g__|                 s__|     51.04|\n",
      "|MGYG000465659|d__Bacteria|   p__Marinisomatota|            g__TCS55|s__TCS55 sp002716525|     38.97|\n",
      "|MGYG000465660|d__Bacteria|p__Verrucomicrobiota|         g__JABJCN01|s__JABJCN01 sp913...|     54.17|\n",
      "+-------------+-----------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(path.join(DATA_FOLDER, \"df_taxonomy.pq\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e88c9",
   "metadata": {
    "id": "646e88c9"
   },
   "outputs": [],
   "source": [
    "# let's get some statistics for the parsed GFFs\n",
    "\n",
    "spark.sql((\n",
    "    \"SELECT type, count(type) as total_rows from features\"\n",
    "    \" GROUP BY type\"\n",
    "    \" ORDER BY total_rows DESC\"\n",
    ")).toPandas().set_index(\"type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab08d8",
   "metadata": {
    "id": "55ab08d8"
   },
   "source": [
    "### Exercise: Scaling Up Data without Changing the Code\n",
    "\n",
    "Let's select these new genera below, totaling to around 38 genomes (~4x larger), for parsing. **Using Axolotl's fastaIO and gff3IO, can you parse those genomes into new DataFrames?**\n",
    "\n",
    "Phylum - Genus\n",
    "\n",
    "- Actinomycetota - Microbacterium (13 genomes)\n",
    "- Bacteroidota - Aquimarina (10 genomes)\n",
    "- Bacillota - Bacillus (9 genomes)\n",
    "- Pseudomonadota - Kordiimonas (6 genomes)\n",
    "\n",
    "\n",
    "**_Assign new variable and table names for the new DataFrames_ --> use contigs_2 for the FASTA files and features_2 for the GFF files, as they will be used in the follow-up blocks**. Hint: use the same code as previously (check boxes before this), but reassign the variables and the input array.\n",
    "\n",
    "Put your code down here (remove the comment tags and fill in the blank '...' with the correct code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6WfFYvcXwlse",
   "metadata": {
    "id": "6WfFYvcXwlse"
   },
   "outputs": [],
   "source": [
    "#included_genera_2 = [\n",
    "# ...\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AP4x3YtYjYuE",
   "metadata": {
    "id": "AP4x3YtYjYuE"
   },
   "outputs": [],
   "source": [
    "#input_fasta_paths_2 = ...\n",
    "\n",
    "#contigs_2 = ...\n",
    "\n",
    "#%time print(contigs_2.df.persist().count())\n",
    "#contigs.df.createOrReplaceTempView(\"contigs_2\")\n",
    "\n",
    "#contigs.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wgvMKUUGj6qK",
   "metadata": {
    "id": "wgvMKUUGj6qK"
   },
   "outputs": [],
   "source": [
    "#input_gff_paths_2 = ...\n",
    "\n",
    "#features_2 = ...\n",
    "\n",
    "#%time print(features_2.df.persist().count())\n",
    "#features.df.createOrReplaceTempView(\"features_2\")\n",
    "\n",
    "#features.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_dspsgZiRzW",
   "metadata": {
    "cellView": "form",
    "id": "P_dspsgZiRzW"
   },
   "outputs": [],
   "source": [
    "# @title spoiler code: uncomment the code below this box and run it if you are stuck\n",
    "\n",
    "def run_spoiler_code():\n",
    "  # this part redefine the input paths, the only thing needs to be changed from the original code\n",
    "  included_genera_2 = [\n",
    "      (\"p__Actinomycetota\", \"g__Microbacterium\"),\n",
    "      (\"p__Bacteroidota\", \"g__Aquimarina\"),\n",
    "      (\"p__Bacillota\", \"g__Bacillus\"),\n",
    "      (\"p__Pseudomonadota\", \"g__Kordiimonas\")\n",
    "  ]\n",
    "  # this part for the FASTAs\n",
    "  input_fasta_paths_2 = [path.join(folder_path, \"*/*.fna\") for folder_path in generate_folder_paths(included_genera_2)]\n",
    "  contigs_2 = FastaIO.loadSmallFiles(\n",
    "      \",\".join(input_fasta_paths_2),\n",
    "      seq_type = \"nucl\",\n",
    "      minPartitions = MIN_PARTITIONS\n",
    "  )\n",
    "  # this part for the GFFs\n",
    "  input_gff_paths_2 = [path.join(folder_path, \"*/*.gff\") for folder_path in generate_folder_paths(included_genera_2)]\n",
    "  features_2 = gff3IO.loadSmallFiles(\n",
    "      \",\".join(input_gff_paths_2),\n",
    "      fasta_path_func = lambda x: x.rsplit(\".\", 1)[0].rsplit(\"_sanntis\", 1)[0] + \".fna\",\n",
    "      minPartitions = MIN_PARTITIONS\n",
    "  )\n",
    "  return contigs_2, features_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XjclC6Zp1Njj",
   "metadata": {
    "id": "XjclC6Zp1Njj"
   },
   "outputs": [],
   "source": [
    "# uncomment and run this if you are stuck\n",
    "#contigs_2, features_2 = run_spoiler_code()\n",
    "#%time print(contigs_2.df.persist().count())\n",
    "#%time print(features_2.df.persist().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971afa87",
   "metadata": {
    "id": "971afa87"
   },
   "source": [
    "### Combining two DataFrames using .union()\n",
    "\n",
    "Spark allows combining DataFrame (must be of the same structure) using DataFrame.union() function (see https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.union.html). Axolotl support the same operations in its DataFrames, **however, note that this will reset the indexes in the combined AxlDF**.\n",
    "\n",
    "Below, we will **combine both the contig's and the feature's dataframes, then _persist them_**, so we can work on the combined version from this point forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cecd12c",
   "metadata": {
    "id": "7cecd12c"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.sequence import NuclSeqDF\n",
    "\n",
    "contigs_combined = NuclSeqDF(contigs.df.union(contigs_2.df), override_idx=True)\n",
    "contigs_combined.df.persist()\n",
    "contigs_combined.df.count()\n",
    "contigs_combined.df.createOrReplaceTempView(\"contigs_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ca787",
   "metadata": {
    "id": "8b9ca787"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.annotation import RawFeatDF\n",
    "\n",
    "features_combined = RawFeatDF(features.df.union(features_2.df), override_idx=True)\n",
    "features_combined.df.persist()\n",
    "features_combined.df.count()\n",
    "features_combined.df.createOrReplaceTempView(\"features_combined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8dac92",
   "metadata": {
    "id": "9f8dac92"
   },
   "source": [
    "#### Let's generate some visualization for the combined data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76s4TvbVAXIv",
   "metadata": {
    "id": "76s4TvbVAXIv"
   },
   "source": [
    "<i>Map contig lengths per genus</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361a00e",
   "metadata": {
    "id": "4361a00e"
   },
   "outputs": [],
   "source": [
    "# map out contig lengths per genus\n",
    "ctg_length_summary = spark.sql((\n",
    "    \"SELECT printf('%s:%s ', Phylum, Genus) as genus, collect_list(log(length)) as ctg_lengths\" # collect contig lengths per genus as a list\n",
    "    \" FROM contigs_combined JOIN taxonomy ON split(seq_id, '_')[0]=taxonomy.Genome\" # join with taxonomy table using genome IDs\n",
    "    \" GROUP BY Phylum, Genus\"\n",
    "    \" ORDER BY Phylum, Genus ASC\"\n",
    ")).toPandas()\n",
    "ctg_length_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L7wm9VeoAjRh",
   "metadata": {
    "id": "L7wm9VeoAjRh"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4));\n",
    "plt.title(\"Contig lengths (log10)\");\n",
    "plt.xticks(rotation=-30, ha=\"left\");\n",
    "plt.boxplot(ctg_length_summary[\"ctg_lengths\"].values, labels=ctg_length_summary[\"genus\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89805358",
   "metadata": {
    "id": "89805358"
   },
   "outputs": [],
   "source": [
    "# map out BGC numbers per genome per genus\n",
    "bgc_num_summary = spark.sql((\n",
    "    \"SELECT genus, collect_list(num_bgcs) as num_bgcs\"\n",
    "    \" FROM (SELECT printf('%s:%s ', Phylum, Genus) as genus, count(type) as num_bgcs\"\n",
    "          \" FROM features_combined JOIN taxonomy ON split(seq_id, '_')[0]=taxonomy.Genome\"\n",
    "          \" WHERE type like 'CLUSTER'\"\n",
    "          \" GROUP BY taxonomy.Genome, Phylum, Genus)\"\n",
    "    \" GROUP BY genus\"\n",
    "    \" ORDER BY genus ASC\"\n",
    ")).toPandas()\n",
    "\n",
    "bgc_num_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wXUjl-gPK8y4",
   "metadata": {
    "id": "wXUjl-gPK8y4"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4));\n",
    "#plt.title(\"Contig lengths (log10)\");\n",
    "plt.xticks(rotation=-30, ha=\"left\");\n",
    "plt.boxplot(bgc_num_summary[\"num_bgcs\"].values, labels=bgc_num_summary[\"genus\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7953c9c1",
   "metadata": {
    "id": "7953c9c1"
   },
   "source": [
    "### Activity 2-3: Extracting BGC and CDS tables from Raw Features table\n",
    "\n",
    "A common data preprocessing activity is transforming (through filtering, joining, function calling) one or more data tables into another. In this case, we take an example of transforming the RawFeatDF (features table) parsed directly from the GFF files into a separate cdsDF (which contain only CDS features data) and bgcDF (which contain only BGC features data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tMAtDnUQNGVA",
   "metadata": {
    "id": "tMAtDnUQNGVA"
   },
   "source": [
    "<i>As a reference, the original RawFeatDF table looks like this:</i> (most of the important information is stored in the 'qualifiers' column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PyANamu_NFLr",
   "metadata": {
    "id": "PyANamu_NFLr"
   },
   "outputs": [],
   "source": [
    "features_combined.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yKPP4MmUNXHe",
   "metadata": {
    "id": "yKPP4MmUNXHe"
   },
   "source": [
    "#### RawFeatDF -> cdsDF\n",
    "\n",
    "<b>cdsDF</b>, as the name suggests, is an Axolotl dataframe class specifically made for storing Coding Sequences (CDS) information. In a typical genome annotation scenario (using GFF or GBK files), CDS information is stored as features, with type == 'CDS', and include information such as locus tag, gene id, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DQi7TwO3N5hk",
   "metadata": {
    "id": "DQi7TwO3N5hk"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.annotation import cdsDF\n",
    "\n",
    "# to 'extract' cdsDF from a RawFeatDF table, we use the fromRawFeatDF() function\n",
    "cds_combined = cdsDF.fromRawFeatDF(features_combined)\n",
    "%time print(cds_combined.df.persist().count()) # persist this for further analysis\n",
    "\n",
    "# this is what cdsDF table will looks like:\n",
    "# the 'locus_tag', 'gene_name', etc. were derived from the original 'qualifiers' column of the RawFeatDF\n",
    "cds_combined.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_gvX6l8nSahS",
   "metadata": {
    "id": "_gvX6l8nSahS"
   },
   "source": [
    "#### utilizing Axolotl's pre-made functions\n",
    "\n",
    "Notice how in the table above, the 'aa_sequence' column seems to be empty. While some annotation pipeline store AA sequence in the feature qualifiers, some don't. In this case, we will need to re-translate based on the location of the CDS and the nucleotide sequences (i.e., codons) taken from the contigs.\n",
    "\n",
    "In Axolotl, the cdsDF class support this function via the <b>cdsDF.translateAAs()</b> function. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IpS8ci5tSZX_",
   "metadata": {
    "id": "IpS8ci5tSZX_"
   },
   "outputs": [],
   "source": [
    "# see how many CDS rows are untranslated\n",
    "cds_combined.df.select(F.isnotnull(F.col(\"aa_sequence\")).alias(\"have_translation\")).groupBy(\"have_translation\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9VxfoGxgVbpz",
   "metadata": {
    "id": "9VxfoGxgVbpz"
   },
   "outputs": [],
   "source": [
    "# let's use Axolotl to translate the AAs. Since the GFF also doesn't provide transl_table value, we will use the most common table = 11\n",
    "# this will take around 1.5 minutes\n",
    "\n",
    "cds_combined_translated = cds_combined.translateAAs(contigs_combined, default_table=11) # assign into a new variable\n",
    "%time cds_combined_translated.df.persist().count() # to kick start the process, we will persist and collect the new DF\n",
    "\n",
    "cds_combined_translated.df.filter(F.isnotnull(F.col(\"aa_sequence\"))).limit(3).toPandas() # let's see how the new table looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AiWS3FV9WEWM",
   "metadata": {
    "id": "AiWS3FV9WEWM"
   },
   "outputs": [],
   "source": [
    "# see how many CDS rows are now translated\n",
    "cds_combined_translated.df.select(F.isnotnull(F.col(\"aa_sequence\")).alias(\"have_translation\")).groupBy(\"have_translation\").count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hG4QfHIve_q2",
   "metadata": {
    "id": "hG4QfHIve_q2"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 6));\n",
    "plt.title(\"CDS lengths\");\n",
    "plt.boxplot(cds_combined_translated.df.select(F.length(F.col(\"aa_sequence\")).alias(\"length\")).toPandas()[\"length\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9FaibOJMWx1N",
   "metadata": {
    "id": "9FaibOJMWx1N"
   },
   "source": [
    "### utilizing Axolotl to perform parallel HMM scanning (using the pyHMMer library)\n",
    "\n",
    "While Axolotl doesn't yet have an extensive list of pre-made functions and pipelines to cover the breadth of analyses people may do in genomics, we implement some most common ones, partly in order to demonstrate how flexible the library is for incorporating external tools, libraries and softwares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oy_tvCKLWxdL",
   "metadata": {
    "id": "oy_tvCKLWxdL"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://storage.googleapis.com/zw_axolotl/jgi_workshop_2024/data/PF00005.hmm\n",
    "!wget https://storage.googleapis.com/zw_axolotl/jgi_workshop_2024/data/PF00005.hmm.h3f\n",
    "!wget https://storage.googleapis.com/zw_axolotl/jgi_workshop_2024/data/PF00005.hmm.h3i\n",
    "!wget https://storage.googleapis.com/zw_axolotl/jgi_workshop_2024/data/PF00005.hmm.h3m\n",
    "!wget https://storage.googleapis.com/zw_axolotl/jgi_workshop_2024/data/PF00005.hmm.h3p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-TE-ZiiRjFlH",
   "metadata": {
    "id": "-TE-ZiiRjFlH"
   },
   "source": [
    "<i>due to time constraints, we will only run the scanning on a limited subset of the cdsDF table</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c34699d",
   "metadata": {
    "id": "4c34699d"
   },
   "outputs": [],
   "source": [
    "from axolotl.app.bgc_analysis.functions import scan_cdsDF\n",
    "\n",
    "cds_example_limited = cdsDF(cds_combined_translated.df.limit(10000), keep_idx=True)\n",
    "\n",
    "hmm_result = scan_cdsDF(cds_example_limited, \"/content/PF00005.hmm\", bit_cutoff=30)\n",
    "\n",
    "%time hmm_result.persist().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y9n9n1AdnQ3c",
   "metadata": {
    "id": "Y9n9n1AdnQ3c"
   },
   "source": [
    "This is what the resulting hits table will looks like. The information provided will be enough to generate a multi-alignment table for all the sequences against the profile HMM (based on locations + gaps and the original CDS sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sXxyIbKyxiqC",
   "metadata": {
    "id": "sXxyIbKyxiqC"
   },
   "outputs": [],
   "source": [
    "hmm_result.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B_6i716Png97",
   "metadata": {
    "id": "B_6i716Png97"
   },
   "source": [
    "#### an extra example, showing the bgcDF class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f6a112",
   "metadata": {
    "id": "34f6a112"
   },
   "outputs": [],
   "source": [
    "from axolotl.data.annotation import bgcDF\n",
    "\n",
    "bgc_combined = bgcDF.fromRawFeatDF(\n",
    "    features_combined,\n",
    "    source_type=\"custom\",\n",
    "    kw_type=\"CLUSTER\",\n",
    "    kw_classes=\"nearest_MiBIG_class\"\n",
    ")\n",
    "%time print(bgc_combined.df.persist().count())\n",
    "\n",
    "bgc_combined.df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee680e7",
   "metadata": {
    "id": "2ee680e7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PnFjEaxzmmxs",
   "metadata": {
    "id": "PnFjEaxzmmxs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
